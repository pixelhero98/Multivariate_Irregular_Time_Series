import os, torch
import crypto_config
from torch import nn
from tqdm import tqdm
from torch.cuda.amp import GradScaler, autocast
from Dataset.fin_dataset import run_experiment
from Latent_Space.latent_vae import LatentVAE
from Model.lladit import LLapDiT
from Model.lladit_utils import (EMA, set_torch, encode_mu_norm, _flatten_for_mask,
                                        make_warmup_cosine, calculate_v_variance,
                                        compute_latent_stats, diffusion_loss,
                                        build_context, flatten_targets,
                                        sample_t_uniform, decode_latents_with_vae
                                        )

# ============================ Training setup ============================

device = set_torch()

train_dl, val_dl, test_dl, sizes = run_experiment(
    data_dir=crypto_config.DATA_DIR,
    date_batching=crypto_config.date_batching,
    dates_per_batch=crypto_config.BATCH_SIZE,
    K=crypto_config.WINDOW,
    H=crypto_config.PRED,
    coverage=crypto_config.COVERAGE
)
print("sizes:", sizes)

# infer dims from one batch
xb0, yb0, meta0 = next(iter(train_dl))
V0, T0 = xb0
B0, N0, K0, Fv = V0.shape
Ft = T0.shape[-1]
H = yb0.shape[-1]
assert Fv == Ft, f"Expected Fv == Ft, got {Fv} vs {Ft}"
print("V:", V0.shape, "T:", T0.shape, "y:", yb0.shape)

# ---- Estimate global latent stats (uses same window-normalization) ----
vae = LatentVAE(
    input_dim=1, seq_len=crypto_config.PRED,
    latent_dim=crypto_config.VAE_LATENT_DIM,
    latent_channel=crypto_config.VAE_LATENT_CHANNELS,
    enc_layers=crypto_config.VAE_LAYERS, enc_heads=crypto_config.VAE_HEADS, enc_ff=crypto_config.VAE_FF,
    dec_layers=crypto_config.VAE_LAYERS, dec_heads=crypto_config.VAE_HEADS, dec_ff=crypto_config.VAE_FF
).to(device)

if os.path.isfile(crypto_config.VAE_CKPT):
    ckpt = torch.load(crypto_config.VAE_CKPT, map_location=device)
    sd = ckpt.get("state_dict", ckpt)
    vae.load_state_dict(sd, strict=False)
    print("Loaded VAE checkpoint:", crypto_config.VAE_CKPT)

vae.eval()
for p in vae.parameters():
    p.requires_grad = False

mu_mean, mu_std = compute_latent_stats(vae, train_dl, device)

# ---- Conditional diffusion model ----
diff_model = LLapDiT(
    data_dim=crypto_config.VAE_LATENT_CHANNELS, hidden_dim=crypto_config.MODEL_WIDTH,
    num_layers=crypto_config.NUM_LAYERS, num_heads=crypto_config.NUM_HEADS,
    predict_type=crypto_config.PREDICT_TYPE, laplace_k=crypto_config.LAPLACE_K, global_k=crypto_config.GLOBAL_K,
    timesteps=crypto_config.TIMESTEPS, schedule=crypto_config.SCHEDULE,
    dropout=crypto_config.DROPOUT, attn_dropout=crypto_config.ATTN_DROPOUT,
    self_conditioning=crypto_config.SELF_COND,
    context_dim=Fv, num_entities=N0, context_len=crypto_config.CONTEXT_LEN,
    lap_mode=crypto_config.LAP_MODE
).to(device)

# ---- Calculate the variance of the v-prediction target ----
v_variance = calculate_v_variance(
    scheduler=diff_model.scheduler,
    dataloader=val_dl,
    vae=vae,
    device=device,
    latent_stats=(mu_mean, mu_std)
)
print(f"calculated V-Prediction Target Variance: {v_variance:.4f}")
print("=========================================================")

ema = EMA(diff_model, decay=crypto_config.EMA_DECAY) if crypto_config.USE_EMA_EVAL else None

scheduler = diff_model.scheduler
optimizer = torch.optim.AdamW(diff_model.parameters(),
                              lr=crypto_config.BASE_LR,
                              weight_decay=crypto_config.WEIGHT_DECAY)
lr_sched = make_warmup_cosine(optimizer, crypto_config.EPOCHS * max(1, len(train_dl)),
                              warmup_frac=crypto_config.WARMUP_FRAC,
                              base_lr=crypto_config.BASE_LR,
                              min_lr=crypto_config.MIN_LR)
scaler = GradScaler(enabled=(device.type == "cuda"))

# ============================ train/val loops ============================
def train_one_epoch(epoch: int):
    diff_model.train()
    running_loss = 0.0
    num_samples = 0
    global global_step

    for xb, yb, meta in train_dl:
        V, T = xb
        mask_bn = meta["entity_mask"]

        cond_summary = build_context(diff_model, V, T, mask_bn, device)
        y_in, batch_ids = flatten_targets(yb, mask_bn, device)
        if y_in is None:
            continue

        cond_summary_flat = cond_summary[batch_ids]  # [Beff,S,Hm]
        mu_norm = encode_mu_norm(
            vae, y_in,
            mu_mean=mu_mean, mu_std=mu_std
        )

        # --------- per-sample classifier-free dropout mask ---------
        Beff = mu_norm.size(0)
        p_drop = float(crypto_config.DROP_COND_P)
        m_cond = (torch.rand(Beff, device=device) >= p_drop)    # True → conditioned
        idx_c = m_cond.nonzero(as_tuple=False).squeeze(1)
        idx_u = (~m_cond).nonzero(as_tuple=False).squeeze(1)

        # shared t / noise / q_sample for the whole batch
        t = sample_t_uniform(scheduler, Beff, device)
        noise = torch.randn_like(mu_norm)
        x_t, eps_true = scheduler.q_sample(mu_norm, t, noise)

        # --------- self-conditioning per branch (optional) ---------
        sc_feat_c = sc_feat_u = None
        use_sc = (epoch >= crypto_config.SELF_COND_START_EPOCH
                  and torch.rand(()) < crypto_config.SELF_COND_P) and crypto_config.SELF_COND
        if use_sc:
            with torch.no_grad():
                if idx_c.numel() > 0:
                    pred_ng_c = diff_model(x_t[idx_c], t[idx_c], cond_summary=cond_summary_flat[idx_c], sc_feat=None)
                    sc_feat_c = scheduler.to_x0(x_t[idx_c], t[idx_c], pred_ng_c, crypto_config.PREDICT_TYPE).detach()
                if idx_u.numel() > 0:
                    pred_ng_u = diff_model(x_t[idx_u], t[idx_u], cond_summary=None, sc_feat=None)
                    sc_feat_u = scheduler.to_x0(x_t[idx_u], t[idx_u], pred_ng_u, crypto_config.PREDICT_TYPE).detach()

        # --------- compute losses on each subset, weighted by fraction ---------
        optimizer.zero_grad(set_to_none=True)
        loss = 0.0

        with autocast(enabled=(device.type == "cuda")):
            if idx_c.numel() > 0:
                loss_c = diffusion_loss(
                    diff_model, scheduler, mu_norm[idx_c], t[idx_c],
                    cond_summary=cond_summary_flat[idx_c], predict_type=crypto_config.PREDICT_TYPE,
                    weight_scheme=crypto_config.LOSS_WEIGHT_SCHEME,
                    minsnr_gamma=crypto_config.MINSNR_GAMMA,
                    sc_feat=sc_feat_c,
                    reuse_xt_eps=(x_t[idx_c], eps_true[idx_c]),
                )
                loss = loss + loss_c * (idx_c.numel() / Beff)

            if idx_u.numel() > 0:
                loss_u = diffusion_loss(
                    diff_model, scheduler, mu_norm[idx_u], t[idx_u],
                    cond_summary=None, predict_type=crypto_config.PREDICT_TYPE,
                    weight_scheme=crypto_config.LOSS_WEIGHT_SCHEME,
                    minsnr_gamma=crypto_config.MINSNR_GAMMA,
                    sc_feat=sc_feat_u,
                    reuse_xt_eps=(x_t[idx_u], eps_true[idx_u]),
                )
                loss = loss + loss_u * (idx_u.numel() / Beff)

        # guard for numerical issues
        if not torch.isfinite(loss):
            print("[warn] non-finite loss detected; skipping step")
            optimizer.zero_grad(set_to_none=True)
            continue

        scaler.scale(loss).backward()
        scaler.unscale_(optimizer)
        if getattr(crypto_config, "GRAD_CLIP", 0.0) and crypto_config.GRAD_CLIP > 0:
            nn.utils.clip_grad_norm_(diff_model.parameters(), crypto_config.GRAD_CLIP)
        scaler.step(optimizer)
        scaler.update()
        if ema is not None:
            ema.update(diff_model)
        lr_sched.step()

        running_loss += float(loss.item()) * Beff
        num_samples += Beff

    return running_loss / max(1, num_samples)


@torch.no_grad()
def validate():
    diff_model.eval()
    total, count = 0.0, 0
    cond_gap_accum, cond_gap_batches = 0.0, 0

    if ema is not None:
        ema.store(diff_model)
        ema.copy_to(diff_model)

    for xb, yb, meta in val_dl:
        V, T = xb
        mask_bn = meta["entity_mask"]

        cond_summary = build_context(diff_model, V, T, mask_bn, device)
        y_in, batch_ids = flatten_targets(yb, mask_bn, device)
        if y_in is None:
            continue
        cond_summary_flat = cond_summary[batch_ids]
        mu_norm = encode_mu_norm(
            vae, y_in,
            mu_mean=mu_mean, mu_std=mu_std
        )

        # main validation loss
        t = sample_t_uniform(scheduler, mu_norm.size(0), device)
        loss = diffusion_loss(
            diff_model, scheduler, mu_norm, t,
            cond_summary=cond_summary_flat, predict_type=crypto_config.PREDICT_TYPE
        )
        total += loss.item() * mu_norm.size(0)
        count += mu_norm.size(0)

        # low-variance cond_gap probe: reuse same (t, x_t, eps) for both branches
        probe_n = min(128, mu_norm.size(0))
        if probe_n > 0:
            mu_p = mu_norm[:probe_n]
            cs_p = cond_summary_flat[:probe_n]
            t_p = sample_t_uniform(scheduler, probe_n, device)
            noise_p = torch.randn_like(mu_p)
            x_t_p, eps_p = scheduler.q_sample(mu_p, t_p, noise_p)

            loss_cond = diffusion_loss(
                diff_model, scheduler, mu_p, t_p,
                cond_summary=cs_p, predict_type=crypto_config.PREDICT_TYPE,
                reuse_xt_eps=(x_t_p, eps_p)
            ).item()

            loss_unco = diffusion_loss(
                diff_model, scheduler, mu_p, t_p,
                cond_summary=None, predict_type=crypto_config.PREDICT_TYPE,
                reuse_xt_eps=(x_t_p, eps_p)
            ).item()

            cond_gap_accum += (loss_unco - loss_cond)
            cond_gap_batches += 1

    if ema is not None:
        ema.restore(diff_model)

    avg_val = total / max(1, count)
    cond_gap = (cond_gap_accum / cond_gap_batches) if cond_gap_batches > 0 else float("nan")
    return avg_val, cond_gap

# ============================ run ============================
skip_with_trained_model = "./ldt/checkpoints/epoch_138_val_0.188933_cond_0.025167269366128103.pt"

if skip_with_trained_model and os.path.exists(skip_with_trained_model):
    print(f"Skipping training. Loading model from: {skip_with_trained_model}")
    ckpt = torch.load(skip_with_trained_model, map_location=device)

    # Load model weights
    diff_model.load_state_dict(ckpt["model_state"])
    print("Loaded model state.")

    # Load EMA weights if they exist in the checkpoint and are enabled
    if ema is not None and "ema_state" in ckpt:
        ema.load_state_dict(ckpt["ema_state"])
        print("Loaded EMA state.")

    # Load stats needed for downstream tasks
    mu_mean = ckpt["mu_mean"].to(device)
    mu_std = ckpt["mu_std"].to(device)

else:
    # --- TRAINING LOOP ---
    if skip_with_trained_model:
        print(f"Model path not found: {skip_with_trained_model}. Starting training from scratch.")

    best_val = float("inf")
    patience = 0
    current_best_path = None
    os.makedirs(crypto_config.CKPT_DIR, exist_ok=True)

    for epoch in tqdm(range(1, crypto_config.EPOCHS + 1), desc="Epochs"):
        train_loss = train_one_epoch(epoch)
        val_loss, cond_gap = validate()
        Z = crypto_config.VAE_LATENT_CHANNELS
        print(f"Epoch {epoch:03d} | train: {train_loss:.6f} (/Z: {train_loss / Z:.6f}) "
              f"| val: {val_loss:.6f} (/Z: {val_loss / Z:.6f}) | cond_gap: {cond_gap:.6f}")

        # checkpoint best (with EMA state)
        if val_loss < best_val:
            best_val = val_loss;
            patience = 0
            if current_best_path and os.path.exists(current_best_path):
                os.remove(current_best_path)

            ckpt_path = os.path.join(
                crypto_config.CKPT_DIR,
                f"epoch_{epoch:03d}_val_{val_loss / Z:.6f}_cond_{cond_gap}.pt"
            )
            save_payload = {
                "epoch": epoch,
                "model_state": diff_model.state_dict(),
                "optimizer_state": optimizer.state_dict(),
                "mu_mean": mu_mean.detach().cpu(),
                "mu_std": mu_std.detach().cpu(),
                "predict_type": crypto_config.PREDICT_TYPE,
                "timesteps": crypto_config.TIMESTEPS,
                "schedule": crypto_config.SCHEDULE,
            }
            if ema is not None:
                save_payload["ema_state"] = ema.state_dict()
                save_payload["ema_decay"] = crypto_config.EMA_DECAY
            torch.save(save_payload, ckpt_path)
            # print("Saved:", ckpt_path)
            current_best_path = ckpt_path
        else:
            patience += 1
            if patience >= crypto_config.EARLY_STOP:
                print("Early stopping.")
                break

# ============================ Decoder finetune + regression eval ============================

def finetune_vae_decoder(
        vae,
        diff_model,
        train_dl,
        val_dl,
        device,
        *,
        mu_mean,
        mu_std,
        config,  # Pass config object for parameters
        ema=None,
        epochs: int = 3,
        lr: float = 1e-4,
        weight_decay: float = 1e-5,
        gen_steps: int = 36,
        guidance_strength: float = 2.0,
        guidance_power: float = 0.3,
        lambda_rec_anchor: float = 0.25,
):
    """
    Fine-tune ONLY the VAE decoder to adapt to the diffusion model's generated latent x0_norm.
    """
    print(f"[decoder-ft(gen)] epochs={epochs}, lr={lr}, steps={gen_steps}, "
          f"guidance={guidance_strength}, power={guidance_power}, anchor={lambda_rec_anchor}")

    # --- Freeze encoder; train decoder only ---
    for p in vae.parameters(): p.requires_grad = False
    for p in vae.decoder.parameters(): p.requires_grad = True
    vae.train()

    # --- Prepare diffusion model (teacher) ---
    diff_model.eval()
    use_ema = (ema is not None)
    if use_ema:
        ema.store(diff_model)
        ema.copy_to(diff_model)

    opt = torch.optim.AdamW(vae.decoder.parameters(), lr=lr, weight_decay=weight_decay)
    scaler = torch.cuda.amp.GradScaler(enabled=(device.type == "cuda"))
    mse = torch.nn.MSELoss()

    def _run_epoch(loader, tag, train: bool):
        vae.train() if train else vae.eval()
        total_gen, total_anchor, total_all, count = 0.0, 0.0, 0.0, 0

        for xb, yb, meta in loader:
            V, T = xb
            mask_bn = meta["entity_mask"]
            cs_full = build_context(diff_model, V, T, mask_bn, device)
            y_true, batch_ids = _flatten_for_mask(yb, mask_bn, device)
            if y_true is None:
                continue
            cs = cs_full[batch_ids]

            Beff, Hcur, Z = y_true.size(0), y_true.size(1), mu_mean.numel()

            # ---- Generate x0_norm in latent space (no grads) ----
            with torch.no_grad():
                x0_norm_gen = diff_model.generate(
                    shape=(Beff, Hcur, Z), steps=gen_steps,
                    guidance_strength=guidance_strength, guidance_power=guidance_power,
                    cond_summary=cs, self_cond=crypto_config.SELF_COND, cfg_rescale=True,
                )

                # ---- Forward decoder on generated latents ----
            with torch.cuda.amp.autocast(enabled=(device.type == "cuda")):
                y_hat_gen = decode_latents_with_vae(
                    vae, x0_norm_gen.detach(), mu_mean=mu_mean, mu_std=mu_std
                )
                loss_gen = mse(y_hat_gen, y_true)

                # ---- Teacher-forced anchor loss ----
                with torch.no_grad():
                    _, mu_enc, _ = vae(y_true)
                y_hat_rec = vae.decoder(mu_enc.detach(), encoder_skips=None)
                loss_anchor = mse(y_hat_rec, y_true)

                loss = loss_gen + lambda_rec_anchor * loss_anchor

            if train:
                opt.zero_grad(set_to_none=True)
                scaler.scale(loss).backward()
                scaler.step(opt)
                scaler.update()

            bs = y_true.size(0)
            total_gen += loss_gen.item() * bs
            total_anchor += loss_anchor.item() * bs
            total_all += loss.item() * bs
            count += bs

        print(f"[decoder-ft(gen)] {tag}  loss_gen={total_gen / max(1, count):.6f}  "
              f"loss_anchor={total_anchor / max(1, count):.6f}  total={total_all / max(1, count):.6f}")

    for e in range(1, epochs + 1):
        _run_epoch(train_dl, f"train@{e}", train=True)
        _run_epoch(val_dl, f"val@{e}", train=False)

    # --- Restore original states ---
    for p in vae.decoder.parameters(): p.requires_grad = False
    if use_ema:
        ema.restore(diff_model)


@torch.no_grad()
def evaluate_regression(
    diff_model, vae, dataloader, device, mu_mean, mu_std, config, ema=None,
    steps: int = 36, guidance_strength: float = 2.0, guidance_power: float = 0.3,
    aggregation_method: str = 'mean',
    quantiles: tuple = (0.1, 0.5, 0.9),
):
    """
    Evaluates the model by generating multiple samples and creating a probabilistic forecast.

    Metrics:
      - MAE / MSE on the aggregated point forecast (mean or median across samples) — exact per-element.
      - CRPS via all-pairs estimator: CRPS = E[|X - y|] - 0.5 * E[|X - X'|].
      - Pinball (quantile) loss for given quantiles:
            L_q(y, ŷ_q) = mean( max(q*(y - ŷ_q), (q-1)*(y - ŷ_q)) )
        where ŷ_q is the sample quantile of the predictive distribution at level q.

    Args:
        aggregation_method: 'mean' or 'median' for the point forecast.
        quantiles: tuple of quantile levels in (0,1), e.g., (0.1, 0.5, 0.9).

    Returns:
        dict with keys:
            crps, mae, mse, num_samples, aggregation, pinball (dict {q: loss})
    """
    if aggregation_method not in ['mean', 'median']:
        raise ValueError("aggregation_method must be either 'mean' or 'median'")
    if not all(0.0 < float(q) < 1.0 for q in quantiles):
        raise ValueError("All quantiles must be in the open interval (0, 1).")

    diff_model.eval()

    # Exact element-wise accumulators for deterministic metrics
    abs_sum, sq_sum, elts = 0.0, 0.0, 0
    # Batch-weighted CRPS accumulator (matches your previous semantics)
    crps_sum, n = 0.0, 0
    # Element-wise accumulators for pinball losses per quantile
    pinball_sums = {float(q): 0.0 for q in quantiles}

    num_samples = config.NUM_EVAL_SAMPLES
    if num_samples <= 1 and aggregation_method == 'median':
        print("Warning: Median aggregation is more meaningful with num_samples > 1.")

    # Use EMA weights if provided
    use_ema = (ema is not None)
    if use_ema:
        ema.store(diff_model)
        ema.copy_to(diff_model)

    warned_no_gt_scale = False  # one-time notice if config asks for GT-scale

    for xb, yb, meta in dataloader:
        V, T = xb
        mask_bn = meta["entity_mask"]

        cond_summary = build_context(diff_model, V, T, mask_bn, device)
        y_in, batch_ids = _flatten_for_mask(yb, mask_bn, device)
        if y_in is None:
            continue
        cs = cond_summary[batch_ids]

        Beff, Hcur, Z = y_in.size(0), y_in.size(1), config.VAE_LATENT_CHANNELS

        # ---- Generate multiple samples in latent space and decode ----

        all_y_hats = []
        for _ in range(num_samples):
            x0_norm = diff_model.generate(
                shape=(Beff, Hcur, Z), steps=steps,
                guidance_strength=guidance_strength, guidance_power=guidance_power,
                cond_summary=cs, self_cond=crypto_config.SELF_COND, cfg_rescale=True,
            )

            if getattr(config, "DECODE_USE_GT_SCALE", False) and not warned_no_gt_scale:
                print("Warning: DECODE_USE_GT_SCALE is True in config, but disabled during evaluation to avoid leakage.")
                warned_no_gt_scale = True

            y_hat_sample = decode_latents_with_vae(
                vae, x0_norm, mu_mean=mu_mean, mu_std=mu_std
            )
            all_y_hats.append(y_hat_sample)

        # Stack samples: [S, B, H, C]
        all_samples = torch.stack(all_y_hats, dim=0)

        # ---- Point forecast via aggregation ----
        if aggregation_method == 'mean':
            point_forecast = all_samples.mean(dim=0)
        else:  # 'median'
            point_forecast = all_samples.median(dim=0).values

        # ---- Deterministic metrics (exact) ----
        res = point_forecast - y_in
        abs_sum += res.abs().sum().item()
        sq_sum  += (res ** 2).sum().item()
        elts    += res.numel()

        # ---- CRPS using all-pairs estimator ----
        # CRPS = E[|X - y|] - 0.5 * E[|X - X'|]
        M = all_samples.shape[0]
        term1 = (all_samples - y_in.unsqueeze(0)).abs().mean(dim=0)  # [B,H,C]

        if M <= 1:
            term2 = torch.zeros_like(term1)
        else:
            diffs = (all_samples.unsqueeze(0) - all_samples.unsqueeze(1)).abs()  # [M,M,B,H,C]
            iu = torch.triu_indices(M, M, offset=1, device=diffs.device)
            diffs_ij = diffs[iu[0], iu[1], ...]                                   # [M*(M-1)/2,B,H,C]
            term2 = (2.0 / (M * (M - 1))) * diffs_ij.mean(dim=0)                  # [B,H,C]

        batch_crps = (term1 - 0.5 * term2).mean().item()
        crps_sum += batch_crps * Beff
        n += Beff

        # ---- Pinball (quantile) loss ----
        for q in quantiles:
            q = float(q)
            y_q = torch.quantile(all_samples, q, dim=0, interpolation="linear")   # [B,H,C]
            diff = y_in - y_q                                                     # note: y - ŷ_q
            # pinball per element
            loss_q = torch.maximum(q * diff, (q - 1.0) * diff)                    # [B,H,C]
            pinball_sums[q] += loss_q.sum().item()

    if use_ema:
        ema.restore(diff_model)

    mae = abs_sum / max(1, elts)
    mse = sq_sum  / max(1, elts)
    crps = crps_sum / max(1, n)
    pinball = {q: (pinball_sums[q] / max(1, elts)) for q in pinball_sums.keys()}

    # Print a concise summary
    qs_fmt = ", ".join(f"{q:.2f}:{pinball[q]:.6f}" for q in sorted(pinball.keys()))
    print(f"[test ({num_samples} samples, aggregation: {aggregation_method})]")
    print(f"  CRPS: {crps:.6f} | MAE: {mae:.6f} | MSE: {mse:.6f} | Pinball[{qs_fmt}]")

    return {
        "crps": crps,
        "mae": mae,
        "mse": mse,
        "pinball": pinball,
        "num_samples": num_samples,
        "aggregation": aggregation_method,
    }


# ---------- Run decoder FT + test regression once training stops ----------
if True:
    # It's good practice to pass the whole config object
    # to avoid long argument lists and make adding new parameters easier.
    if crypto_config.DECODER_FT_EPOCHS > 0:
        torch.manual_seed(42)
        finetune_vae_decoder(
            vae, diff_model, train_dl, val_dl, device,
            mu_mean=mu_mean, mu_std=mu_std,
            config=crypto_config,  # Pass config object
            ema=ema,
            epochs=crypto_config.DECODER_FT_EPOCHS,
            lr=crypto_config.DECODER_FT_LR,  # CRITICAL BUG FIX
            gen_steps=crypto_config.GEN_STEPS,
            guidance_strength=crypto_config.GUIDANCE_STRENGTH,
            guidance_power=crypto_config.GUIDANCE_POWER,
            lambda_rec_anchor=crypto_config.DECODER_FT_ANCHOR
        )

    # Evaluate conditional regression on test set
    evaluate_regression(
        diff_model, vae, test_dl, device, mu_mean, mu_std,
        config=crypto_config,  # Pass config object
        steps=crypto_config.GEN_STEPS,
        guidance_strength=crypto_config.GUIDANCE_STRENGTH,
        guidance_power=crypto_config.GUIDANCE_POWER,
        aggregation_method='mean',
        quantiles=(0.1, 0.5, 0.9)
    )



Epoch 145 | train: 0.547695 (/Z: 0.027385) | val: 2.844080 (/Z: 0.142204) | cond_gap: 0.019379
Epochs:  10%|▉         | 145/1500 [1:26:21<13:26:59, 35.73s/it]
Epoch 146 | train: 0.546357 (/Z: 0.027318) | val: 2.856353 (/Z: 0.142818) | cond_gap: 0.020252
Early stopping.
[decoder-ft(gen)] epochs=10, lr=0.0002, steps=100, guidance=2.0, power=0.3, anchor=0.1
[decoder-ft(gen)] train@1  loss_gen=1.129563  loss_anchor=0.467976  total=1.176360
[decoder-ft(gen)] val@1  loss_gen=0.394408  loss_anchor=0.316122  total=0.426020
[decoder-ft(gen)] train@2  loss_gen=0.995314  loss_anchor=0.697697  total=1.065083
[decoder-ft(gen)] val@2  loss_gen=0.377575  loss_anchor=0.321284  total=0.409704
[decoder-ft(gen)] train@3  loss_gen=0.986587  loss_anchor=0.731868  total=1.059774
[decoder-ft(gen)] val@3  loss_gen=0.372502  loss_anchor=0.323379  total=0.404840
[decoder-ft(gen)] train@4  loss_gen=0.983500  loss_anchor=0.743435  total=1.057844
[decoder-ft(gen)] val@4  loss_gen=0.370070  loss_anchor=0.323104  total=0.402380
[decoder-ft(gen)] train@5  loss_gen=0.981765  loss_anchor=0.749969  total=1.056762
[decoder-ft(gen)] val@5  loss_gen=0.369059  loss_anchor=0.323533  total=0.401412



import math
from typing import List, Optional, Tuple
import torch
import torch.nn as nn

# ============================LLapDiT utils============================
def set_torch():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    torch.backends.cuda.matmul.allow_tf32 = True
    if device.type == "cuda":
        torch.set_float32_matmul_precision("high")
    return device


def sample_t_uniform(scheduler, n: int, device: torch.device) -> torch.Tensor:
    return torch.randint(0, scheduler.timesteps, (n,), device=device)


def make_warmup_cosine(optimizer, total_steps, warmup_frac=0.05, base_lr=5e-4, min_lr=1e-6):
    warmup_steps = max(1, int(total_steps * warmup_frac))

    def lr_lambda(step):
        if step < warmup_steps:
            return (step + 1) / max(1, warmup_steps)
        progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)
        return max(min_lr / max(base_lr, 1e-12), 0.5 * (1.0 + math.cos(math.pi * progress)))

    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)


def _cosine_alpha_bar(t, s=0.008):
    """Continuous-time alpha_bar(t) from Nichol & Dhariwal (t in [0,1])."""
    return torch.cos((t + s) / (1 + s) * math.pi / 2) ** 2


class NoiseScheduler(nn.Module):
    """
    Diffusion utilities with precomputed buffers and a DDIM sampler.
    Supports 'linear' or 'cosine' schedules and epsilon-/v-/x0-parameterization.
    """

    def __init__(
            self,
            timesteps: int = 1000,
            schedule: str = "cosine",
            beta_start: float = 1e-4,
            beta_end: float = 2e-2,
    ) -> None:
        super().__init__()
        self.timesteps = int(timesteps)
        if schedule not in {"linear", "cosine"}:
            raise ValueError(f"Unknown schedule: {schedule}")
        self.schedule = schedule

        # ---- build betas ----
        if schedule == "linear":
            betas = torch.linspace(beta_start, beta_end, self.timesteps, dtype=torch.float32)
            betas = betas.clamp(min=1e-8, max=0.999)
        else:
            # cosine ᾱ(t) from Nichol & Dhariwal; turn into alphas and then betas
            ts = torch.linspace(0, 1, self.timesteps + 1, dtype=torch.float32)
            abar = _cosine_alpha_bar(ts)
            abar = abar / abar[0]  # ᾱ(0) = 1
            alphas = torch.ones(self.timesteps, dtype=torch.float32)
            alphas[1:] = abar[1:self.timesteps] / abar[0:self.timesteps - 1]
            betas = (1.0 - alphas).clone()
            betas[1:] = betas[1:].clamp(min=1e-8, max=0.999)

        betas[0] = 0.0
        self.register_buffer("betas", betas)
        alphas = 1.0 - betas
        self.register_buffer("alphas", alphas)
        alpha_bars = torch.cumprod(alphas, dim=0)
        self.register_buffer("alpha_bars", alpha_bars)
        ab = alpha_bars.clamp(0.0, 1.0)
        self.register_buffer("sqrt_alphas", torch.sqrt(alphas.clamp(0.0, 1.0)))
        self.register_buffer("sqrt_alpha_bars", torch.sqrt(ab))
        self.register_buffer("sqrt_one_minus_alpha_bars", torch.sqrt((1.0 - ab).clamp(0.0, 1.0)))

    @torch.no_grad()
    def timesteps_desc(self):
        return torch.arange(self.timesteps - 1, -1, -1, dtype=torch.long, device=self.alpha_bars.device)

    def _gather(self, buf: torch.Tensor, t: torch.Tensor):
        t = t.clamp(min=0, max=self.timesteps - 1).to(device=buf.device, dtype=torch.long)
        return buf.gather(0, t)

    def q_sample(self, x0: torch.Tensor, t: torch.Tensor, noise: torch.Tensor = None):
        if noise is None:
            noise = torch.randn_like(x0)
        sqrt_ab = self._gather(self.sqrt_alpha_bars, t).view(-1, *([1] * (x0.dim() - 1)))
        sqrt_1_ab = self._gather(self.sqrt_one_minus_alpha_bars, t).view(-1, *([1] * (x0.dim() - 1)))
        x_t = sqrt_ab * x0 + sqrt_1_ab * noise
        return x_t, noise

    def pred_x0_from_eps(self, x_t, t, eps):
        alpha = self._gather(self.sqrt_alpha_bars, t).view(-1, *([1] * (x_t.dim() - 1)))
        sigma = self._gather(self.sqrt_one_minus_alpha_bars, t).view(-1, *([1] * (x_t.dim() - 1)))
        return (x_t - sigma * eps) / (alpha + 1e-12)

    def pred_eps_from_x0(self, x_t, t, x0):
        alpha = self._gather(self.sqrt_alpha_bars, t).view(-1, *([1] * (x_t.dim() - 1)))
        sigma = self._gather(self.sqrt_one_minus_alpha_bars, t).view(-1, *([1] * (x_t.dim() - 1)))
        return (x_t - alpha * x0) / (sigma + 1e-12)

    def pred_x0_from_v(self, x_t, t, v):
        alpha = self._gather(self.sqrt_alpha_bars, t).view(-1, *([1] * (x_t.dim() - 1)))
        sigma = self._gather(self.sqrt_one_minus_alpha_bars, t).view(-1, *([1] * (x_t.dim() - 1)))
        return alpha * x_t - sigma * v

    def pred_eps_from_v(self, x_t, t, v):
        alpha = self._gather(self.sqrt_alpha_bars, t).view(-1, *([1] * (x_t.dim() - 1)))
        sigma = self._gather(self.sqrt_one_minus_alpha_bars, t).view(-1, *([1] * (x_t.dim() - 1)))
        return sigma * x_t + alpha * v

    def v_from_eps(self, x_t, t, eps):
        alpha = self._gather(self.sqrt_alpha_bars, t).view(-1, *([1] * (x_t.dim() - 1)))
        sigma = self._gather(self.sqrt_one_minus_alpha_bars, t).view(-1, *([1] * (x_t.dim() - 1)))
        return (eps - sigma * x_t) / (alpha + 1e-12)

    def to_x0(self, x_t, t, pred, param_type: str):
        if param_type == "eps":
            return self.pred_x0_from_eps(x_t, t, pred)
        elif param_type == "v":
            return self.pred_x0_from_v(x_t, t, pred)
        elif param_type == "x0":
            return pred
        else:
            raise ValueError("param_type must be 'eps', 'v', or 'x0'")

    def to_eps(self, x_t, t, pred, param_type: str):
        if param_type == "eps":
            return pred
        elif param_type == "v":
            return self.pred_eps_from_v(x_t, t, pred)
        elif param_type == "x0":
            return self.pred_eps_from_x0(x_t, t, pred)
        else:
            raise ValueError("param_type must be 'eps', 'v', or 'x0'")

    @torch.no_grad()
    def ddim_sigma(self, t, t_prev, eta: float):
        ab_t = self._gather(self.alpha_bars, t)
        ab_prev = self._gather(self.alpha_bars, t_prev)
        sigma = eta * torch.sqrt((1.0 - ab_prev) / (1.0 - ab_t)) * torch.sqrt(1.0 - ab_t / (ab_prev + 1e-12))
        return sigma.view(-1)

    @torch.no_grad()
    def ddim_step_from(self, x_t, t, t_prev, pred, param_type: str, eta: float = 0.0, noise: torch.Tensor = None):
        if noise is None:
            noise = torch.randn_like(x_t)
        ab_prev = self._gather(self.alpha_bars, t_prev).view(-1, *([1] * (x_t.dim() - 1)))
        sigma = self.ddim_sigma(t, t_prev, eta).view(-1, *([1] * (x_t.dim() - 1)))

        x0_pred = self.to_x0(x_t, t, pred, param_type)
        eps_pred = self.to_eps(x_t, t, pred, param_type)

        dir_coeff = torch.clamp((1.0 - ab_prev) - sigma ** 2, min=0.0)
        dir_xt = torch.sqrt(dir_coeff) * eps_pred
        x_prev = torch.sqrt(ab_prev) * x0_pred + dir_xt + sigma * noise
        return x_prev


# ============================ Laplace pole logging ============================
def iter_laplace_bases(module):
    from Model.laptrans import LearnableLaplacianBasis
    for m in module.modules():
        if isinstance(m, LearnableLaplacianBasis):
            yield m


@torch.no_grad()
def log_pole_health(modules: List[nn.Module], log_fn, step: int, tag_prefix: str = ""):
    alphas, omegas = [], []
    for mod in modules:
        for lap in iter_laplace_bases(mod):
            tau = torch.nn.functional.softplus(lap._tau) + 1e-3
            alpha = lap.s_real.clamp_min(lap.alpha_min) * tau  # [k]
            omega = lap.s_imag * tau  # [k]
            alphas.append(alpha.detach().cpu())
            omegas.append(omega.detach().cpu())
    if not alphas:
        return
    alpha_cat = torch.cat([a.view(-1) for a in alphas])
    omega_cat = torch.cat([o.view(-1) for o in omegas])
    log_fn({f"{tag_prefix}alpha_mean": alpha_cat.mean().item(),
            f"{tag_prefix}alpha_min": alpha_cat.min().item(),
            f"{tag_prefix}alpha_max": alpha_cat.max().item(),
            f"{tag_prefix}omega_abs_mean": omega_cat.abs().mean().item()}, step=step)


def _print_log(metrics: dict, step: int, csv_path: str = None):
    msg = " | ".join(f"{k}={v:.4g}" for k, v in metrics.items())
    print(f"[poles] step {step:>7d} | {msg}")
    if csv_path is not None:
        import csv, os
        head = ["step"] + list(metrics.keys())
        row = [step] + [metrics[k] for k in metrics]
        write_header = not os.path.exists(csv_path)
        with open(csv_path, "a", newline="") as f:
            w = csv.writer(f)
            if write_header:
                w.writerow(head)
            w.writerow(row)


# ============================ VAE Latent stats helpers ============================
def flatten_targets(yb: torch.Tensor, mask_bn: torch.Tensor, device: torch.device) -> Tuple[torch.Tensor, torch.Tensor]:
    """yb: [B,N,H] -> y_in: [Beff,H,1], batch_ids: [Beff] mapping to B for cond rows"""
    y = yb.to(device)
    B, N, Hcur = y.shape
    y_flat = y.reshape(B * N, Hcur).unsqueeze(-1)  # [B*N, H, 1]
    m_flat = mask_bn.to(device).reshape(B * N)
    if not m_flat.any():
        return None, None
    y_in = y_flat[m_flat]
    batch_ids = torch.arange(B, device=device).unsqueeze(1).expand(B, N).reshape(B * N)[m_flat]
    return y_in, batch_ids


@torch.no_grad()
def _flatten_for_mask(yb, mask_bn, device):
    y_in, batch_ids = flatten_targets(yb, mask_bn, device)
    return y_in, batch_ids


# ============================ EMA Weights (for evaluation) ============================
class EMA:
    def __init__(self, model, decay=0.999):
        self.decay = decay
        self.shadow = {n: p.detach().clone() for n, p in model.named_parameters() if p.requires_grad}

    @torch.no_grad()
    def update(self, model):
        for n, p in model.named_parameters():
            if p.requires_grad:
                self.shadow[n].lerp_(p.detach(), 1.0 - self.decay)

    def store(self, model):
        self._backup = {n: p.detach().clone() for n, p in model.named_parameters() if p.requires_grad}

    def copy_to(self, model):
        for n, p in model.named_parameters():
            if p.requires_grad:
                p.data.copy_(self.shadow[n].data)

    def restore(self, model):
        for n, p in model.named_parameters():
            if p.requires_grad:
                p.data.copy_(self._backup[n].data)

    def state_dict(self):
        return {k: v.cpu() for k, v in self.shadow.items()}

    def load_state_dict(self, sd):
        for k, v in sd.items():
            if k in self.shadow:
                self.shadow[k] = v.clone()


# ============================ Latent helpers ============================
def simple_norm(mu: torch.Tensor, mu_mean: torch.Tensor, mu_std: torch.Tensor, clip_val: float = None) -> torch.Tensor:
    """
    Apply dataset-level per-dimension z-score to latent means.
      - mu: [B,L,Z]
      - mu_mean: [Z]
      - mu_std: [Z]
    Returns normalized latents with the same shape.
    """
    mu_mean = mu_mean.to(device=mu.device, dtype=mu.dtype).view(1,1,-1)
    mu_std  = mu_std.to(device=mu.device, dtype=mu.dtype).clamp_min(1e-6).view(1,1,-1)
    x = (mu - mu_mean) / mu_std
    if clip_val is not None:
        x = x.clamp(-clip_val, clip_val)
    return x


def invert_simple_norm(x: torch.Tensor, mu_mean: torch.Tensor, mu_std: torch.Tensor) -> torch.Tensor:
    """
    Inverse of simple_norm.
      - x: [B,L,Z]
    """
    mu_mean = mu_mean.to(device=x.device, dtype=x.dtype).view(1,1,-1)
    mu_std  = mu_std.to(device=x.device, dtype=x.dtype).view(1,1,-1)
    return x * mu_std + mu_mean


def normalize_cond_per_batch(cs: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:
    """z-score over (B,S) for each feature dim; keeps gradients. cs: [B,S,Hc]"""
    m = cs.mean(dim=(0, 1), keepdim=True)
    v = cs.var(dim=(0, 1), keepdim=True, unbiased=False)
    return (cs - m) / (v.sqrt() + eps)


@torch.no_grad()
def compute_latent_stats(vae, dataloader, device):
    """
    Compute dataset-level latent mean/std on *raw* μ.
    """
    all_mu = []
    for (_, y, meta) in dataloader:
        m = meta["entity_mask"]
        B, N, H = y.shape
        y_flat = y.reshape(B * N, H).unsqueeze(-1)
        m_flat = m.reshape(B * N)
        if not m_flat.any():
            continue
        y_in = y_flat[m_flat].to(device)
        _, mu, _ = vae(y_in)  # [Beff, L, Z]
        all_mu.append(mu.detach().cpu())

    mu_cat = torch.cat(all_mu, dim=0)  # [sum(Beff), L, Z]
    mu_mean = mu_cat.mean(dim=(0, 1)).to(device)
    mu_std  = mu_cat.std(dim=(0, 1), correction=0).clamp_min(1e-6).to(device)
    return mu_mean, mu_std


def decode_latents_with_vae(vae, x0_norm: torch.Tensor,
                            mu_mean: torch.Tensor, mu_std: torch.Tensor) -> torch.Tensor:
    """
    Invert normalization and decode with the VAE decoder (no encoder skips).
      - x0_norm: [B,L,Z] normalized latent from diffusion
    Returns:
      x_hat: [B,L,1]  (same feature domain your VAE was trained on)
    """
    mu_est = invert_simple_norm(x0_norm, mu_mean, mu_std)
    x_hat = vae.decoder(mu_est, encoder_skips=None)
    return x_hat


def build_context(model, V: torch.Tensor, T: torch.Tensor,
                  mask_bn: torch.Tensor, device: torch.device, norm: bool = True) -> torch.Tensor:
    """Returns normalized cond_summary: [B,S,Hm]"""
    with torch.no_grad():
        series_diff = T.permute(0, 2, 1, 3).to(device)  # [B,K,N,F]
        series = V.permute(0, 2, 1, 3).to(device)  # [B,K,N,F]
        mask_bn = mask_bn.to(device)
        cond_summary, _ = model.context(x=series, ctx_diff=series_diff, entity_mask=mask_bn)
        if norm:
            cond_summary = normalize_cond_per_batch(cond_summary)
    return cond_summary


def encode_mu_norm(vae, y_in: torch.Tensor, *,
                   mu_mean: torch.Tensor, mu_std: torch.Tensor) -> torch.Tensor:
    """VAE encode then globally z-score; returns [Beff, H, Z]"""
    with torch.no_grad():
        _, mu, _ = vae(y_in)
        mu_norm = simple_norm(mu, mu_mean, mu_std, clip_val=None)
        mu_norm = torch.nan_to_num(mu_norm, nan=0.0, posinf=0.0, neginf=0.0)
    return mu_norm


def diffusion_loss(model, scheduler, x0_lat_norm: torch.Tensor, t: torch.Tensor,
                   *, cond_summary: Optional[torch.Tensor], predict_type: str = "v",
                   weight_scheme: str = "none", minsnr_gamma: float = 5.0,
                   sc_feat: Optional[torch.Tensor] = None,
                   reuse_xt_eps: Optional[Tuple[torch.Tensor, torch.Tensor]] = None) -> torch.Tensor:
    """
    MSE on v/eps with optional MinSNR weighting.
    If reuse_xt_eps=(x_t, eps_true) is provided, use that instead of re-sampling.
    """
    if reuse_xt_eps is None:
        noise = torch.randn_like(x0_lat_norm)
        x_t, eps_true = scheduler.q_sample(x0_lat_norm, t, noise)
    else:
        x_t, eps_true = reuse_xt_eps

    pred = model(x_t, t, cond_summary=cond_summary, sc_feat=sc_feat)
    target = eps_true if predict_type == "eps" else scheduler.v_from_eps(x_t, t, eps_true)

    err = (pred - target).pow(2)  # [B,H,Z]
    per_sample = err.mean(dim=1).sum(dim=1)  # [B]

    if weight_scheme == 'none':
        return per_sample.mean()
    elif weight_scheme == 'weighted_min_snr':
        abar = scheduler.alpha_bars[t]
        snr = abar / (1.0 - abar).clamp_min(1e-8)
        gamma = minsnr_gamma
        w = torch.minimum(snr, torch.as_tensor(gamma, device=snr.device, dtype=snr.dtype))
        w = w / (snr + 1.0)
        return (w.detach() * per_sample).mean()


@torch.no_grad()
def calculate_v_variance(scheduler, dataloader, vae, device, latent_stats):
    """
    Calculates the variance of the v-prediction target over a given dataloader.
    """
    all_v_targets = []
    print("Calculating variance of v-prediction target...")

    # Unpack the pre-computed latent statistics
    mu_mean, mu_std = latent_stats

    # Loop through the validation set
    for xb, yb, meta in dataloader:
        # This block is the same as in your validate() function
        # It gets the normalized latent variable 'mu_norm' which is the x0 for diffusion
        y_in, _ = flatten_targets(yb, meta["entity_mask"], device)
        if y_in is None:
            continue

        mu_norm = encode_mu_norm(
            vae, y_in,
            mu_mean=mu_mean,
            mu_std=mu_std
        )

        # Now, simulate the process for creating the 'v' target
        # 1. Sample random timesteps
        t = sample_t_uniform(scheduler, mu_norm.size(0), device)

        # 2. Create the noise that would be added
        noise = torch.randn_like(mu_norm)

        # 3. Apply the forward process to get the noised latent x_t
        x_t, _ = scheduler.q_sample(mu_norm, t, noise)

        # 4. Calculate the ground-truth 'v' from x_t and the noise
        v_target = scheduler.v_from_eps(x_t, t, noise)
        all_v_targets.append(v_target.detach().cpu())

    # Concatenate all batches and compute the final variance
    if not all_v_targets:
        print("Warning: No valid data found to calculate variance.")
        return float('nan')

    all_v_targets_cat = torch.cat(all_v_targets, dim=0)
    v_variance = all_v_targets_cat.var(correction=0).item()

    return v_variance
